---
title: "Overlaps"
date: "Updated: `r format(Sys.Date(), format='%B %d %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 4
---

---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```


```{r message=FALSE, warning=FALSE}
# Libraries
pacman::p_load(tidyverse, countrycode, sf, state, htmltools, htmlwidgets, urltools, janitor, DT, RColorBrewer, plotly, readxl)
tld <- read_csv(here::here("data/tld-country.csv"))
tld_from_ip <- read_csv(here::here("data/tld_from_ip.csv"))
shapefile <- read_sf(here::here("data/TM_WORLD_BORDERS_SIMPL-0.3.shp"))
```

# Global

```{r}
# Data (Aug 5, 2021)
df <- read_csv(here::here("data/jonas/beacon-public.csv"))

canada.states <- 
  c("Alberta", "British Columbia", "Labrador", "Manitoba", "New Brunswick", "Newfoundland", "Nova Scotia", "Nunavut", "North West Terr.", "Ontario", "Prince Edward Is.", "QuÃ©bec (Province)", "Saskatchewan", "Yukon")

# correcting some oai urls
df <- 
  df %>% 
  mutate(oai_url = if_else(str_detect(oai_url, "^//"), str_c("http:", oai_url), oai_url))


# getting domain and tld
df <-  
  bind_cols(
    df,
    df %>% pull(oai_url) %>% domain() %>% tld_extract()
  ) %>% 
  mutate(country_consolidated = str_to_lower(country_consolidated) %>% str_trim()) %>% 
  left_join(tld, by = c("country_consolidated" = "tld"))

# Cleaning countries + Mapping countries to continents
df <-
  df %>% 
  mutate(
    country = country_clean,
    ojs_v2 = str_detect(version, "^2"),
    ojs_v3 = str_detect(version, "^3"),
    version_clean = case_when(
      ojs_v2 ~ "Version 2",
      ojs_v3 ~ "Version 3",
      TRUE ~ "Other"
    )
  ) %>% 
  mutate(
    country = if_else(country == "Washington (State)", "United States", country),
    country = if_else(str_detect(country, "United States|New York|District of Columbia"), "United States", country),
    country = if_else(country %in% state.name, "United States", country),
    country = if_else(country %in% canada.states, "Canada", country),
    country = if_else(str_detect(country, "China"), "China", country),
    country = if_else(str_detect(country, "Armenia"), "Armenia", country),
    country = if_else(str_detect(country, "Georgia"), "Georgia", country),
    country = if_else(str_detect(country, "Australia|New South Wales|Queensland|Victoria"), "Australia", country),
    country = if_else(str_detect(country, "England|British"), "United Kingdom", country),
    country = if_else(str_detect(country, "Russia|Soviet Union"), "Russia", country),
    country = if_else(str_detect(country, "Palestine"), "Palestine", country),
    country = if_else(country == "Korea (South)", "South Korea", country)
  ) %>% 
  select(-country_clean)

continents <-
  df %>%
  pull(country) %>% 
  countrycode(origin = "country.name", destination = "continent")

df <- df %>% bind_cols(continents)

# rename last column to `continent`
names(df)[length(names(df))] <- "continent"

df <-
  df %>% 
  filter(total_record_count > 0) %>%
  filter(application == "ojs") %>%
  # filter((record_count_2017 + record_count_2018 + record_count_2019 + record_count_2020 + record_count_2016) >= 100) %>%
  filter(record_count_2020 >= 5) %>% 
  distinct(oai_url, repository_name, set_spec, .keep_all = T)
```


```{r, eval = F}
df %>% glimpse()
```


```{r, eval=F}
# rough - extract domains for gscholar

df %>%
  separate(oai_url, c("url1", "url2"), "\n") %>% 
  remove_empty() %>% 
  rename(oai_url = url1) %>% 
  filter(!str_detect(oai_url, "\\?page=")) %>% 
  transmute(
    country, total_record_count, context_name,
    url = str_replace(oai_url, "index/oai", set_spec),
    url = str_replace(url, "^https://", ""),
    url = str_replace(url, "^http://", ""),
    url = str_replace(url, "^www108.", ""),
    url = str_replace(url, "^www5.", ""),
    url = str_replace(url, "^www3.", ""),
    url = str_replace(url, "^www2.", ""),
    url = str_replace(url, "^www.", ""),
  ) %>%
  bind_rows(
    df %>%
      separate(oai_url, c("url1", "url2"), "\n") %>% 
      remove_empty() %>% 
      rename(oai_url = url1) %>% 
      filter(str_detect(oai_url, "\\?page=")) %>% 
      transmute(
        country, total_record_count, context_name,
        url = str_replace(oai_url, "oai$", set_spec),
        url = str_replace(url, "^https://", ""),
        url = str_replace(url, "^http://", ""),
        url = str_replace(url, "^www108.", ""),
        url = str_replace(url, "^www5.", ""),
        url = str_replace(url, "^www3.", ""),
        url = str_replace(url, "^www2.", ""),
        url = str_replace(url, "^www.", ""),
      ), .
  ) %>% 
  distinct() #%>% 
  #write_csv(here::here("data/gscholar_urls.csv"))
```


```{r, fig.retina=4}
df_gscholar <- 
  read_csv(here::here("scripts/gscholar_urls_mapped.csv")) %>% 
  select(-result_json) %>% 
  mutate(n_results = parse_integer(n_results))

df_gscholar <-
  df %>%
  separate(oai_url, c("url1", "url2"), "\n") %>% 
  remove_empty() %>% 
  rename(oai_url = url1) %>% 
  filter(!str_detect(oai_url, "\\?page=")) %>% 
  transmute(
    country, total_record_count, context_name,
    url = str_replace(oai_url, "index/oai", set_spec),
    url = str_replace(url, "^https://", ""),
    url = str_replace(url, "^http://", ""),
    url = str_replace(url, "^www108.", ""),
    url = str_replace(url, "^www5.", ""),
    url = str_replace(url, "^www3.", ""),
    url = str_replace(url, "^www2.", ""),
    url = str_replace(url, "^www.", ""),
  ) %>%
  bind_rows(
    df %>%
      separate(oai_url, c("url1", "url2"), "\n") %>% 
      remove_empty() %>% 
      rename(oai_url = url1) %>% 
      filter(str_detect(oai_url, "\\?page=")) %>% 
      transmute(
        country, total_record_count, context_name,
        url = str_replace(oai_url, "oai$", set_spec),
        url = str_replace(url, "^https://", ""),
        url = str_replace(url, "^http://", ""),
        url = str_replace(url, "^www108.", ""),
        url = str_replace(url, "^www5.", ""),
        url = str_replace(url, "^www3.", ""),
        url = str_replace(url, "^www2.", ""),
        url = str_replace(url, "^www.", ""),
      ), .
  ) %>% 
  distinct() %>% 
  inner_join(df_gscholar, by = "url")

df_gscholar %>% 
  mutate(present = n_results > 0) %>%
  #tabyl(present) %>% 
  filter(present) %>% 
  count(country) %>% 
  arrange(-n) %>% 
  mutate(country = fct_inorder(country) %>% fct_rev()) %>% 
  head(20) %>% 
  ggplot(aes(country, n)) +
  geom_col() +
  scale_y_continuous(breaks = scales::breaks_width(1000)) +
  theme_minimal() +
  coord_flip() +
  labs(x = "Country", y = "Total journals")
  

df_gscholar %>%
  summarize_if(is.numeric, ~ mean(., na.rm = TRUE))

df_gscholar %>% glimpse()

2552351/19810

df_gscholar %>%
  left_join(read_csv(here::here("scripts/gscholar_citations.csv")), by = "url") %>% 
  arrange(-n_citations) %>% 
  filter(n_results > 0) %>% 
  count(n_citations > 0)

1276/19810
```


```{r}
# find where domains work but urls dont
df_temp <-
  df_gscholar %>% 
  select(journal_name = context_name, journal_url = url, ojs_article_count = total_record_count, scholar_article_count = n_results) %>%
  mutate(
    present_in_scholar = if_else(scholar_article_count > 0, 1L, 0L) %>% replace_na(0L),
    scholar_article_count = replace_na(scholar_article_count, 0L)
  )

df_temp <-
  bind_cols(
    df_temp,
    domain = df_temp %>% pull(journal_url) %>% domain()
  )

df_temp %>% glimpse()

df_temp %>% 
  group_by(domain) %>% 
  summarize(present = mean(present_in_scholar, na.rm = T)) %>% summary()
  arrange(present) %>% 
  filter(present > 0)

df_temp %>% 
  filter(domain == "arcticreview.no") %>% 
  select(journal_url, present_in_scholar)
```


```{r, fig.retina=4}
# histograms

# number of articles
df_gscholar %>%
  left_join(read_csv(here::here("scripts/gscholar_citations.csv")), by = "url") %>%
  mutate(n_results = pmin(n_results, 500)) %>% 
  ggplot(aes(n_results)) +
  geom_histogram(binwidth = 10, color = "black", alpha = 0.75) +
  scale_x_continuous(labels = c("0", "100", "200", "300", "400", "500+")) +
  hrbrthemes::theme_ipsum() +
  labs(
    x = "Number of articles",
    y = "Number of journals"
  )

# number of citations on first gscholar page
df_gscholar %>%
  left_join(read_csv(here::here("scripts/gscholar_citations.csv")), by = "url") %>%
  mutate(n_citations = pmin(n_citations, 500)) %>% 
  ggplot(aes(n_citations)) +
  geom_histogram(binwidth = 10, color = "black", alpha = 0.75) +
  scale_x_continuous(labels = c("0", "100", "200", "300", "400", "500+")) +
  hrbrthemes::theme_ipsum() +
  labs(
    x = "Number of citations on first page",
    y = "Number of journals"
  )

# for anurag
df_gscholar %>% 
  select(journal_name = context_name, journal_url = url, ojs_article_count = total_record_count, scholar_article_count = n_results) %>%
  mutate(
    present_in_scholar = if_else(scholar_article_count > 0, 1L, 0L) %>% replace_na(0L),
    scholar_article_count = replace_na(scholar_article_count, 0L)
  ) %>% glimpse()
  write_csv(here::here("data/gscholar_ojs_mapping_06.12.2022.csv"))
  
12953/19810
```



## WoS data

```{r, fig.retina=4}
df_wos <-
  read_table(here::here("data/overlaps/wos_data_simon.txt"), na = c("NULL", "NA", "")) %>% 
  clean_names() %>% 
  remove_empty() %>% 
  filter(str_detect(issn, "^[0-9]{4}-[0-9]{3}[0-9xX]$")) %>% 
  distinct(issn) %>% 
  drop_na(issn)

# count
df %>% 
  mutate(
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  inner_join(df_wos, by = "issn")

# overlap by country
df %>% 
  mutate(
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  inner_join(df_wos, by = "issn") %>% 
  count(country) %>% 
  arrange(-n) %>% 
  head(10) %>% 
  mutate(country = fct_inorder(country) %>% fct_rev()) %>% 
  ggplot(aes(country, n)) +
  geom_col() +
  hrbrthemes::theme_ipsum() +
  coord_flip() +
  labs(
    x = "Country", y = "Number of journals"
  )

279/22810 # ojs with unique issn
279/24510 # wos
```



```{r}
# Scopus data
df_scopus <-
  read_excel(here::here("data/overlaps/scopus_nov2021.xlsx")) %>%
  clean_names() %>% 
  remove_empty() %>% 
  select(issn = print_issn, e_issn, contains("cite_score"), everything()) %>%
  mutate(issn = if_else(is.na(issn), e_issn, issn)) %>%
  # mutate(issn = if_else(!is.na(e_issn), e_issn, issn)) %>%
  mutate_at(vars(contains("cite_score")), ~ as.numeric(.)) %>% 
  mutate(
    #issn = str_extract(issn, "[^\n]+"),
    issn = str_replace(issn, "-", "")
  ) %>%
  distinct(issn, .keep_all = T) %>% 
  drop_na(issn)


df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+"),
    issn = str_replace(issn, "-", "")
  ) %>%
  inner_join(df_scopus, by = "issn")

df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+"),
    issn = str_replace(issn, "-", "")
  ) %>%
  inner_join(df_scopus, by = "issn") %>%
  drop_na(country) %>% 
  count(country) %>% 
  arrange(-n) %>% 
  head(10) %>% 
  mutate(country = fct_inorder(country) %>% fct_rev()) %>% 
  ggplot(aes(country, n)) +
  geom_col() +
  hrbrthemes::theme_ipsum() +
  coord_flip() +
  labs(
    x = "Country", y = "Number of journals"
  )


1307/22810 # ojs
1307/41934 # scopus
```



```{r}
# df
df


# Dimensions data
df_dimensions <-
  read_excel(here::here("data/overlaps/journals_dimensions.xlsx")) %>%
  clean_names() %>% 
  remove_empty() %>% 
  select(issn = issn_print, e_issn = issn_e, everything()) %>%
  mutate(issn = if_else(issn == "NULL", e_issn, issn)) %>%
  # mutate(issn = if_else(e_issn != "NULL", e_issn, issn)) %>%
  mutate_at(vars(contains("cite_score")), ~ as.numeric(.)) %>% 
  distinct(issn, .keep_all = T) %>% 
  drop_na(issn)

df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  inner_join(df_dimensions, by = "issn")

df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  inner_join(df_dimensions, by = "issn") %>%
  drop_na(country) %>% 
  count(country) %>% 
  arrange(-n) %>% 
  head(10) %>% 
  mutate(country = fct_inorder(country) %>% fct_rev()) %>% 
  ggplot(aes(country, n)) +
  geom_col() +
  hrbrthemes::theme_ipsum() +
  coord_flip() +
  labs(
    x = "Country", y = "Number of journals"
  )

9114/22810 # ojs
9114/72906 # dimensions

df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  anti_join(df_dimensions, ., by = "issn") %>% 
  count(publisher_name) %>% 
  arrange(-n)
```



```{r}
# EBSCO Host
df_ebsco <-
  read_excel(here::here("data/overlaps/ebscohost.xls")) %>%
  clean_names() %>% 
  remove_empty() %>% 
  distinct(issn) %>% 
  drop_na(issn)

df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  inner_join(df_ebsco, by = "issn")

df %>% 
  drop_na(issn) %>%
  transmute(
    country,
    issn = str_extract(issn, "[^\n]+")
  ) %>%
  inner_join(df_ebsco, by = "issn") %>%
  drop_na(country) %>% 
  count(country) %>% 
  arrange(-n) %>% 
  head(10) %>% 
  mutate(country = fct_inorder(country) %>% fct_rev()) %>% 
  ggplot(aes(country, n)) +
  geom_col() +
  hrbrthemes::theme_ipsum() +
  coord_flip() +
  labs(
    x = "Country", y = "Number of journals"
  )

771/22810
771/17874
```

