---
title: "Predatory Publishing Results"
date: "Updated: `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 4
---


```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.retina = 4)
```


```{r message=FALSE, warning=FALSE}
# Libraries
pacman::p_load(tidyverse, countrycode, sf, state, htmltools, htmlwidgets, urltools, janitor, DT, RColorBrewer, plotly, readxl, writexl)
tld <- read_csv(here::here("data/tld-country.csv"))
tld_from_ip <- read_csv(here::here("data/tld_from_ip.csv"))
```



```{r}
# Data (May 17, 2021)
df <- read_csv(here::here("data/jonas/report-latest.csv"))

canada.states <- 
  c("Alberta", "British Columbia", "Labrador", "Manitoba", "New Brunswick", "Newfoundland", "Nova Scotia", "Nunavut", "North West Terr.", "Ontario", "Prince Edward Is.", "QuÃ©bec (Province)", "Saskatchewan", "Yukon")

# correcting some oai urls
df <- 
  df %>% 
  mutate(oai_url = if_else(str_detect(oai_url, "^//"), str_c("http:", oai_url), oai_url))


# getting domain and tld
df <-  
  bind_cols(
    df,
    df %>% pull(oai_url) %>% domain() %>% tld_extract()
  ) %>% 
  mutate(country_consolidated = str_to_lower(country_consolidated) %>% str_trim()) %>% 
  left_join(tld, by = c("country_consolidated" = "tld"))

# Cleaning countries + Mapping countries to continents
df <-
  df %>% 
  mutate(
    country = country_clean,
    ojs_v2 = str_detect(version, "^2"),
    ojs_v3 = str_detect(version, "^3"),
    version_clean = case_when(
      ojs_v2 ~ "Version 2",
      ojs_v3 ~ "Version 3",
      TRUE ~ "Other"
    )
  ) %>% 
  mutate(
    country = if_else(country == "Washington (State)", "United States", country),
    country = if_else(str_detect(country, "United States|New York|District of Columbia"), "United States", country),
    country = if_else(country %in% state.name, "United States", country),
    country = if_else(country %in% canada.states, "Canada", country),
    country = if_else(str_detect(country, "China"), "China", country),
    country = if_else(str_detect(country, "Armenia"), "Armenia", country),
    country = if_else(str_detect(country, "Georgia"), "Georgia", country),
    country = if_else(str_detect(country, "Australia|New South Wales|Queensland|Victoria"), "Australia", country),
    country = if_else(str_detect(country, "England|British"), "United Kingdom", country),
    country = if_else(str_detect(country, "Russia|Soviet Union"), "Russia", country),
    country = if_else(str_detect(country, "Palestine"), "Palestine", country),
    country = if_else(country == "Korea (South)", "South Korea", country)
  ) %>% 
  select(-country_clean)

continents <-
  df %>%
  pull(country) %>% 
  countrycode(origin = "country.name", destination = "continent")

df <- df %>% bind_cols(continents)

# rename last column to `continent`
names(df)[length(names(df))] <- "continent"

df <-
  df %>% 
  filter(total_record_count > 0) %>% 
  mutate(
    record_count_from_2019 = record_count_2019 + record_count_2020 + record_count_2021,
    active = (record_count_from_2019 >= 10),
    active2019 = (record_count_2019 >= 5),
    active2020 = (record_count_2020 >= 5),
    active2021 = (record_count_2021 >= 5)
  ) %>%
  distinct(oai_url, repository_name, set_spec, .keep_all = T)
```


```{r, eval=FALSE}
df %>% summarize_all(~ sum(is.na(.)))
df %>% glimpse()
df %>% drop_na(country) %>% count(active)
```



```{r}
# predatory matching

pred_list <- 
  read_csv("/home/saurabh/Everything/GitHub/beall-live/data/df_weekly_publishers.csv") %>% 
  bind_rows(read_csv("/home/saurabh/Everything/GitHub/beall-live/data/df_weekly_standalone.csv")) %>% 
  clean_names() %>% 
  remove_empty()

pred_list <-
  bind_cols(
    pred_list,
    pred_list %>% pull(url) %>% domain() %>% tld_extract()
  ) %>% 
  relocate(domain) %>% 
  mutate(
    domain = str_replace(domain, "^www.", ""),
    alive = (status == 200),
    alive = if_else(is.na(alive), F, alive)
  ) %>% 
  distinct(domain, .keep_all = T)

df_pred <- 
  df %>%
  mutate(
    domain = str_replace(domain, "^www.", "")
  ) %>%
  inner_join(pred_list, by = "domain") %>%
  relocate(domain)


# 127 out of 2741 publisher domains (4.6% domains are in the beacon)
# 684 journals (372 active, 312 inactive) [1.4% active and 2.1% inactive OJS journals]. 118 of these are in DOAJ.
# US and India
```

&nbsp;

```{r}
# merging in doaj data
df_doaj <- 
  read_excel(here::here("data/kevin/kevin_doaj.xlsx")) %>% clean_names() %>% 
  remove_empty()

df_doaj <-
  bind_cols(
    df_doaj,
    df_doaj %>% pull(journal_url) %>% domain() %>% tld_extract()
  ) %>%
  mutate(
    platform = platform_host_or_aggregator %>% str_trim() %>% str_to_upper(),
    journal_title = journal_title %>% str_trim() %>% str_to_upper(),
    publisher = publisher %>% str_trim() %>% str_to_upper(),
    country = country_of_publisher %>% str_trim(),
    issn = journal_eissn_online_version %>% str_trim(),
    ojs = str_detect(platform, "OJS|NEPJOL|AJOL|SEER|OPEN JOURNAL SYSTEM"),
    domain = str_replace(domain, "^www.", ""),
    doaj = TRUE
  ) %>% 
  filter(ojs)

df_pred <-
  df_pred %>%
  left_join(df_doaj %>% distinct(domain, doaj), by = "domain") %>% 
  mutate(doaj = if_else(is.na(doaj), FALSE, doaj))
```


### OJS journals matching Beall's List

```{r}
table1 <-
  df_pred %>%
  select(country, context_name, url, active, alive, doaj) %>% 
  arrange(country, context_name) 

table1 %>%
  mutate(url = str_c('<a href="', url, '" target="_blank">', url, '</a>')) %>% 
  datatable(escape = F)
```


&nbsp;

### Domains matching Beall's List with OJS journal count

```{r}
table2 <-
  df_pred %>% 
  count(domain, name = "journals") %>% 
  arrange(-journals) 

table2 %>% 
  mutate(domain = str_c('<a href=http://"', domain, '" target="_blank">', domain, '</a>')) %>% 
  datatable(escape = F)
```


&nbsp;

### OJS journals matching Beall's list by country

```{r}
table3 <- df_pred %>% count(country, name = "journals") %>% arrange(-journals)

datatable(table3)
```


&nbsp;

### Active/Alive/DOAJ Breakdown for 684 OJS journals matching Beall's list

```{r}
table4 <- df_pred %>% count(active, alive, doaj, name = "journals")

table4 %>% knitr::kable()
```

```{r, eval=F}
# writing all tables to excel 
sheets <- list(
  "OJS journals matching Bealls List" = table1, 
  "Domains matching Bealls List with OJS journal count" = table2,
  "OJS journals matching Bealls List by country" = table3,
  "Active-Alive-DOAJ Breakdown for 684 OJS journals matching Bealls List" = table4
) 

write_xlsx(sheets, here::here("data/predatory/results/beall_ojs_matching_results.xlsx"))
```



```{r}
df_pred %>%
  
```

