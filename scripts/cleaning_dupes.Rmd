---
title: "Getting lists of duplicates"
date: "Updated: `r Sys.Date()`"
output:
  html_document:
    toc: false
---


```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.retina = 4)
```


```{r message=FALSE, warning=FALSE}
# Libraries
pacman::p_load(tidyverse, countrycode, sf, state, htmltools, htmlwidgets, urltools, janitor, DT, RColorBrewer, plotly)

# Data
df <- read_csv(here::here("data/beacon-latest/beacon.csv"))
tld <- read_csv(here::here("data/tld-country.csv"))
tld_from_ip <- read_csv(here::here("data/tld_from_ip.csv"))

canada.states <- 
  c("Alberta", "British Columbia", "Labrador", "Manitoba", "New Brunswick", "Newfoundland", "Nova Scotia", "Nunavut", "North West Terr.", "Ontario", "Prince Edward Is.", "QuÃ©bec (Province)", "Saskatchewan", "Yukon")
```


```{r}
# correcting some oai urls
df <- 
  df %>% 
  mutate(oai_url = if_else(str_detect(oai_url, "^//"), str_c("http:", oai_url), oai_url))
```


```{r}
# getting domain and tld
df <- 
  bind_cols(
    df,
    df %>% pull(oai_url) %>% domain() %>% tld_extract()
  ) %>% 
  left_join(tld_from_ip, by = "domain") %>%
  mutate(
    tld = if_else(!is.na(tld_from_ip), tld_from_ip, tld)
  ) %>%
  left_join(tld, by = "tld")
```


```{r}
# Cleaning countries + Mapping countries to continents
df <-
  df %>% 
  mutate(
    country = if_else(is.na(country), country_clean, country),
    ojs_v2 = str_detect(version, "^ojs2/2"),
    ojs_v3 = str_detect(version, "^ojs2/3"),
    version_clean = case_when(
      ojs_v2 ~ "Version 2",
      ojs_v3 ~ "Version 3",
      TRUE ~ "Other"
    )
  ) %>% 
  mutate(
    country = if_else(country == "Washington (State)", "United States", country),
    country = if_else(str_detect(country, "United States|New York|District of Columbia"), "United States", country),
    country = if_else(country %in% state.name, "United States", country),
    country = if_else(country %in% canada.states, "Canada", country),
    country = if_else(str_detect(country, "China"), "China", country),
    country = if_else(str_detect(country, "Armenia"), "Armenia", country),
    country = if_else(str_detect(country, "Georgia"), "Georgia", country),
    country = if_else(str_detect(country, "Australia|New South Wales|Queensland|Victoria"), "Australia", country),
    country = if_else(str_detect(country, "England|British"), "United Kingdom", country),
    country = if_else(str_detect(country, "Russia|Soviet Union"), "Russia", country),
    country = if_else(str_detect(country, "Palestine"), "Palestine", country),
    country = if_else(country == "Korea (South)", "South Korea", country)
  ) %>% 
  select(-country_clean)

continents <-
  df %>%
  pull(country) %>% 
  countrycode(origin = "country.name", destination = "continent")

df <- df %>% bind_cols(continents)

# rename last column to `continent`
names(df)[length(names(df))] <- "continent"

rm(tld, tld_from_ip)

df %>% filter(total_record_count > 0) %>% summarise_all(~ mean(is.na(.)))

duplicated_issns <- df %>% drop_na(issn) %>% get_dupes(issn) %>% distinct(issn) %>% pull(issn)
```



```{r}
df %>% 
  filter(!(issn %in% duplicated_issns))

df %>%
  filter(is.na(issn) & total_record_count > 0) %>%
  drop_na(repository_name, admin_email, set_spec) %>%
  arrange(repository_name, admin_email, set_spec, desc(last_oai_response)) %>% 
  relocate(last_oai_response, total_record_count, record_count_2019) %>% 
  distinct(repository_name, admin_email, set_spec, .keep_all = TRUE)

df %>%
  filter(is.na(issn) & total_record_count > 0) %>%
  drop_na(repository_name, admin_email, set_spec) %>%
  get_dupes(repository_name, admin_email, set_spec) %>%
  arrange(repository_name, admin_email, set_spec, desc(last_oai_response)) %>% 
  select(last_oai_response, dupe_count, total_record_count, record_count_2019, repository_name, admin_email, set_spec, domain, oai_url) %>% 
  group_by(repository_name, admin_email, set_spec) %>% 
  mutate(
    names = str_c("total_record_", row_number())
  ) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = c(repository_name, admin_email, set_spec), names_from = names, values_from = total_record_count) %>% 
  relocate(total_record_1, total_record_2) %>% 
  filter(total_record_1 < total_record_2) %>% 
  remove_empty() %>% 
  left_join(
    df %>% distinct(repository_name, admin_email, set_spec, .keep_all = T),
    by = c("repository_name", "admin_email", "set_spec")
  ) %>% 
  relocate(total_record_1, total_record_2, repository_name, admin_email, set_spec, domain, oai_url) %>% 
  arrange(repository_name, admin_email, set_spec) %>% 
  rename(latest_record_count = total_record_1, penultimate_record_count = total_record_2) 
  # writexl::write_xlsx("record_count_latest_version_issue_03.26.21.xlsx")

18/458
```



Noting dupes:

```{r}
# df %>% 
#   drop_na(issn) %>% get_dupes(issn) %>% arrange(issn, desc(dupe_count)) %>% count(issn)
#   write_csv(here::here("beacon_duplicates", "duplicates_by_issn.csv"))
# 
# df %>% 
#   drop_na(issn) %>% get_dupes(issn, repository_name) %>% arrange(issn, repository_name, desc(dupe_count)) %>% 
#   write_csv(here::here("beacon_duplicates", "duplicates_by_issn_reponame.csv"))
# 
# df %>% 
#   drop_na(issn) %>% get_dupes(issn, domain) %>% arrange(issn, domain, desc(dupe_count)) %>% 
#   write_csv(here::here("beacon_duplicates", "duplicates_by_issn_domain.csv"))
# 
# df %>% 
#   drop_na(issn) %>% 
#   get_dupes(issn, repository_name, set_spec) %>% arrange(issn, repository_name, set_spec, desc(dupe_count)) %>% 
#   write_csv(here::here("beacon_duplicates", "duplicates_by_issn_reponame_setspec.csv"))
```


```{r}
# df %>% 
#   filter(is.na(issn) & total_record_count > 0) %>% 
#   drop_na(repository_name, admin_email) %>% 
#   get_dupes(repository_name, admin_email) %>% arrange(repository_name, admin_email, desc(dupe_count)) %>% 
#   write_csv(here::here("beacon_duplicates", "duplicates_reponame_email (no issn).csv"))
# 
# df %>% 
#   filter(is.na(issn) & total_record_count > 0) %>% 
#   drop_na(repository_name, admin_email, set_spec) %>% 
#   get_dupes(repository_name, admin_email, set_spec) %>% arrange(repository_name, admin_email, set_spec, desc(dupe_count)) %>% 
#   write_csv(here::here("beacon_duplicates", "duplicates_reponame_email_setspec (no issn).csv"))
```






```{r}
# Stats for John #

df_world <-
  df %>%
  filter(!(issn %in% duplicated_issns)) %>% 
  filter(record_count_2019 >= 5) %>% 
  drop_na(country) %>%   
  group_by(country, continent) %>% 
  summarize(
    ojs_v2 = sum(ojs_v2, na.rm = T),
    ojs_v3 = sum(ojs_v3, na.rm = T)
  ) %>% 
  ungroup() %>% 
  transmute(
    country,
    continent,
    ojs_v2,
    ojs_v3,
    total = ojs_v2 + ojs_v3
  ) %>% 
  mutate(
    total = replace_na(total, 0),
    ojs_v2 = replace_na(ojs_v2, 0),
    ojs_v3 = replace_na(ojs_v3, 0)
  )


df_world %>% 
  summarise(
    total = sum(total, na.rm = TRUE),
    ojs_v2 = sum(ojs_v2, na.rm = TRUE),
    ojs_v3 = sum(ojs_v3, na.rm = TRUE)
  )

df_world %>% arrange(-total) %>% select(-continent)

df %>%
  filter(record_count_2019 >= 5 & country == "Indonesia" & ojs_v2) %>% 
  count(domain, name = "ojs_v2_journals") %>% 
  arrange(-ojs_v2_journals) %>% 
  datatable()
```



